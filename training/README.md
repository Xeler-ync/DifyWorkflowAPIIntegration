# LLM Embeddings Template (Chinese Hotel FAQ Demo)

This template trains **word embeddings** (Word2Vec / FastText) on a small **Chinese hotel-domain** corpus,
then builds a **semantic FAQ retrieval** demo (mean-pooled sentence embeddings + cosine similarity).
Use it for your presentation (算法部分：词向量学习) and for a simple backend & workflow integration.

## Quick Start

### 1) Create a virtual environment (recommended)
```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# macOS / Linux
source .venv/bin/activate
```

### 2) Install dependencies
```bash
pip install -r requirements.txt
```

> **Note**: `faiss-cpu` is optional and not used in the default scripts. If installation fails on your OS, you can skip it.

### 3) Prepare & train
```bash
# 1) Clean & tokenize corpus (reads data/corpus.txt, writes data/corpus_tokenized.txt)
python scripts/prep_tokenize.py

# 2) Train Word2Vec (and FastText) -> models/*.model
python scripts/train_w2v.py
```

### 4) Build retrieval index and try the interactive demo
```bash
# Build FAQ vectors -> models/faq_index.npz
python scripts/build_retrieval.py

# Run a small interactive console search
python scripts/search_demo.py
```

Type a query like:
```
可以提前入住吗
```
You should see Top-5 FAQ matches with scores printed out.

---

## Project Layout
```
llm-embeddings-template/
├─ data/
│  ├─ corpus.txt              # example hotel-domain sentences (UTF-8; edit/replace with your own)
│  ├─ corpus_tokenized.txt    # generated by prep_tokenize.py (space-separated tokens)
│  ├─ faq.csv                 # small example FAQ knowledge base
│  └─ user_dict.txt           # optional: domain terms to improve jieba segmentation
├─ models/                    # trained models + retrieval index
├─ scripts/
│  ├─ utils.py                # normalization, tokenize_zh, sent_embed, etc.
│  ├─ prep_tokenize.py        # clean + tokenize corpus.txt
│  ├─ train_w2v.py            # train Word2Vec / FastText
│  ├─ inspect_neighbors.py    # print similar words (for screenshots)
│  ├─ build_retrieval.py      # build sentence embeddings for FAQ and save index
│  └─ search_demo.py          # interactive console retrieval demo
├─ requirements.txt
└─ README.md
```

## Tips
- Put important phrases into `data/user_dict.txt` (一行一个，比如“延迟退房”).  
- If key words are OOV (out-of-vocabulary), try `fasttext.model` in `build_retrieval.py` and `search_demo.py`.
- For better results, replace `data/corpus.txt` with **2k–20k** domain sentences.

## License
MIT — feel free to adapt for course/demo use.


---

## Run the FastAPI server

Expose the retrieval demo as an HTTP API (for Dify or other workflow tools).

### 1) Ensure models and index exist (or let the API build the index on start)
```bash
python scripts/prep_tokenize.py
python scripts/train_w2v.py
# optional but recommended:
python scripts/build_retrieval.py
```

### 2) Launch the server (dev mode with auto-reload)
```bash
python -m uvicorn scripts.api:app --reload --port 8000
```

### 3) Try it
- Health check: http://127.0.0.1:8000/ping
- Search (GET): http://127.0.0.1:8000/search?q=可以提前入住吗&k=5
- Search (POST JSON):
```bash
curl -X POST http://127.0.0.1:8000/search   -H "Content-Type: application/json"   -d '{"q":"可以提前入住吗","k":5}'
```

> We added `scripts/__init__.py` so `scripts.api:app` imports cleanly in uvicorn.


---

## Data generation & extras

### Quickly create 3k corpus + 100 FAQ
```bash
python scripts/gen_data.py
```

Then run the standard pipeline:
```bash
python scripts/prep_tokenize.py
python scripts/train_w2v.py
python scripts/inspect_neighbors.py
python scripts/build_retrieval.py
python scripts/tsne_plot.py
```

### Optional: FAISS (if available on your OS)
```bash
pip install faiss-cpu
python scripts/build_retrieval_faiss.py
python scripts/search_faiss_demo.py
```
